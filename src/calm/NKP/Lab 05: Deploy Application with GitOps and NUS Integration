From code-server (bastion VM) create a simple nginx container application:
--------------------------------------------------------------------------
mkdir workshop
cd workshop
mkdir arief
cd arief

kubectx --> choose kommander cluster
export HARBOR_ADDRESS="$(kubectl -n kommander get kommandercluster host-cluster -o jsonpath='{.status.ingress.address}')"
echo $HARBOR_ADDRESS 

vi Dockerfile #change ip address to your harbor registry ip address
from 10.38.106.41:5000/proxy_cache/nginx 
copy index.html /usr/share/nginx/html

vi index.html
<HTML>
<BODY>
<H1>
<CENTER>
LANDING PAGE ..... Arief Pribadi
</CENTER>
</H1>
</BODY>
</HTML>



docker login -u nutanix -p nx2Tech974! https://$HARBOR_ADDRESS:5000/ #change ip address to your harbor registry ip address
docker build -f Dockerfile -t arief-nginx . # -->  If Failed, check endpoint healthy in harbor dashboard  
docker image ls 
docker tag arief-nginx:latest $HARBOR_ADDRESS:5000/library/arief-nginx:latest
docker image ls
docker push $HARBOR_ADDRESS:5000/library/arief-nginx
from harbor dashboard examine Projects - library - repositories

Create new Namespace and Create credential for harbor access:
cd /home/nutanix

kubectx --> make sure you use commander-cluster cluster context
export HARBOR_ADDRESS="$(kubectl -n kommander get kommandercluster host-cluster -o jsonpath='{.status.ingress.address}')"
echo $HARBOR_ADDRESS 

kubectx --> make sure you use managed-cluster cluster context
kubectl get ns
kubectl create ns arief-nginx #use your name 

kubens --> make sure you choose yourname-nginx namespace

REGISTRY_USERNAME="nutanix"
REGISTRY_PASSWORD="nx2Tech974!" #change password to your cluster password

echo $REGISTRY_USERNAME : $REGISTRY_PASSWORD  # --> make sure username and password correct !!!

kubectl -n arief-nginx create secret generic harbor-registry-credentials \
--from-literal username=$REGISTRY_USERNAME \
--from-literal password=$REGISTRY_PASSWORD \
--from-file=/etc/docker/certs.d/$HARBOR_ADDRESS:5000/ca.crt

From Managed Cluster Kubernetes Dashboard:

Choose yourname namespace
Create new resource - Create from Form

Name              :  arief-nginx
container image   :  10.38.106.41:5000/library/arief-nginx # change to your harbor registry ip address
number of pods    :  1
service           :  external
port              :  80
target port       :  80
Show advanced option
image pull secret :  harbor-registry-credentials

While waiting you can check from harbor dashboard Projects - Library - arief-nginx - Artifacts --> Check it last pull time and date
Check deployment running
Check Services created - click external endpoint --> the web application shown

Create more complex application with persistent volue:
------------------------------------------------------
From managed-cluster kubernetes dashboard --> examine installed storage classes: nutanix-volume

Choose your name-nginx namespace from kubernetes dashboard

Apply below manifest using Create New Resources - Create from input in kubernetes dashboard

*** MAKE SURE YOU CHANGE THE IMAGE URL IN DEPLOYMENT MANIFEST INTO YOUR HARBOR PROXY_CACHE URL ***

*** Change every "arief" name into your name in the manifest and change every harbor IP "10.38.106.41:5000" into your harbor IP address ***

mySQL Application:
------------------
apiVersion: v1
kind: Secret
metadata:
  name: arief-mysql-pass                # Change to yourname (not "arief")
  type: Opaque
stringData:
  password: nutanix/4u
---
apiVersion: v1
kind: Service
metadata:
  name: arief-wordpress-mysql            # Change to yourname (not "arief")
  labels:
    app: wordpress
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
  clusterIP: None
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: arief-mysql-pv-claim            # Change to yourname (not "arief")
  labels:
    app: wordpress
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: arief-wordpress-mysql            # Change to yourname (not "arief")
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: mysql
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: mysql
    spec:
      containers:
        - image: 10.38.106.41:5000/proxy_cache/mysql:8.0    # Change the IP into your harbor proxy_cache URL
          name: arief-mysql
          env:
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: arief-mysql-pass    # Change to yourname (not "arief")
                  key: password
            - name: MYSQL_DATABASE
              value: wordpress
            - name: MYSQL_USER
              value: wordpress
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: arief-mysql-pass            # Change to yourname (not "arief")
                  key: password
          ports:
            - containerPort: 3306
              name: mysql
          volumeMounts:
            - name: arief-mysql-persistent-storage
              mountPath: /var/lib/mysql
      volumes:
        - name: arief-mysql-persistent-storage      # Change to yourname (not "arief")
          persistentVolumeClaim:
            claimName: arief-mysql-pv-claim          # Change to yourname (not "arief")
      imagePullSecrets:
        - name: harbor-registry-credentials

Check mySQL Deployment:
  - Click the Persistent Volume Claims option on the sidebar menu to see the persistent volume created for MySQL --> status bound
    * Copy the volume name (ex: pvc-85eef0b5-8f84-4231-8401-20eabe87d6c1)
  - Secret --> mysql-pass exist
  - Services --> wordpress-mysql exist
  - Deployment --> Status running
  - Using kubernetes dashboard - Pods - wordpress-mysql-xxx --> Click 3 button menu and click Exec
    * you are inside the mysql container --> ls /var/lib/mysql --> this is where the files stored in Nutanix CSI Storage
  - From Prism Central - Infrastructure - Storage - Volume Groups
    * search your volume name (ex: pvc-85eef0b5-8f84-4231-8401-20eabe87d6c1)
    * this is your new create volume group mapped to your new PVC

Wordpress Application:
----------------------
apiVersion: v1
kind: Service
metadata:
  name: arief-wordpress              # Change to yourname (not "arief")
  labels:
    app: wordpress
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: frontend
  clusterIP: None
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: arief-wp-pv-claim              # Change to yourname (not "arief")
  labels:
    app: wordpress
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: arief-wordpress                # Change to yourname (not "arief")
  labels:
    app: wordpress
spec:
  replicas: 1
  selector:
    matchLabels:
      app: wordpress
      tier: frontend
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: frontend
    spec:
      containers:
      - image: 10.54.82.43:5000/proxy_cache/wordpress:apache   # Change the IP into your harbor proxy_cache URL
        name: wordpress
        env:
        - name: WORDPRESS_DB_HOST
          value: arief-wordpress-mysql    # Change to yourname (not "arief")
        - name: WORDPRESS_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: arief-mysql-pass      # Change to yourname (not "arief")
              key: password
        - name: WORDPRESS_DB_USER
          value: wordpress
        ports:
        - containerPort: 80
          name: wordpress
        volumeMounts:
        - name: arief-wordpress-persistent-storage        # Change to yourname (not "arief")
          mountPath: /var/www/html
      volumes:
      - name: arief-wordpress-persistent-storage          # Change to yourname (not "arief")
        persistentVolumeClaim:
          claimName: arief-wp-pv-claim                    # Change to yourname (not "arief")
      imagePullSecrets:
        - name: harbor-registry-credentials


Check WordPress Deployment:
  - Click the Persistent Volume Claims option on the sidebar menu to see the persistent volume created for WordPress --> status bound
  - Services --> wordpress-mysql exist
  - Deployment --> Status running
  - Using kubernetes dashboard - Pods - wordpress-mysql-xxx --> Click 3 button menu and click Exec
    * you are inside the mysql container --> ls /var/lib/mysql --> you will find wordress database directory there
    * ls /var/lib/mysql/wordpress/ --> to explore more the wordpress database files
    * mysql -u wordpress -p 
        - password: nutanix/4u
    * show databases;
    * use wordpress;
    * show tables;  # row still empty

Create Ingress to access wordpress:
-----------------------------------

*** Take a note that mysql and wordpress services above is "Headless" means clusterIP: None (Cant be expose by NodePort or LoadBalancer) ***

From kubernetes dashboard:
  - Click service - ingress classes from left pane menu --> there is a traefik ingress available in the cluster
  - Click service - ingresses --> you will see no ingresses in this namespace (yet)
  - Switch to All-Namespace to find trafik end point IP:
    * find "kommander-traefik" --> identify kommander-traefik services External End Point (Ex: 10.38.106.53)
  - Switch back to yourname-namespace (ex: Arief-nginx)
  - Click New Resources - Create from input and input below manifest

*** MAKE SURE YOU SWITCH YOU NAMESPACE INTO YOURNAME NAMESPACE (EX: ARIEF-NGINX) ***

Traefik Ingress Manifest
------------------------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: arief-wordpress              # Change to yourname (not "arief")
spec:
  ingressClassName: kommander-traefik
  rules:
    - host: wordpress.arief.com       # Change to yourname domain name (not "arief.com")
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: arief-wordpress  # Change to yourname (not "arief")
                port:
                  number: 80

Click service - ingresses --> make sure there is a wordpress ingress

--> From AUTOAD VM Add DNS A Host record of "wordpress" to kommander-traefik services External End Point (Ex: 10.38.106.53) in arief.com zone 
--> or add wordpress and its kommander-traefik services External End Point (Ex: 10.38.106.53) into your laptop host file

** Check wordpress application https://wordpress.arief.com
    - Create username and password 
    - Install workdpress
    - Login using your new created login

** Check again the database - from you kubernetes dashboard
  - Using kubernetes dashboard - Pods - wordpress-mysql-xxx --> Click 3 button menu and click Exec
    * you are inside the mysql container --> ls /var/lib/mysql --> you will find wordress database directory there
    * ls /var/lib/mysql/wordpress/ --> to explore more the wordpress database files
    * mysql -u wordpress -p 
        - password: nutanix/4u
    * show databases;
    * use wordpress;
    * show tables;     # row now populated proven the database has been established from the application container.

Deploy Application using GitOps Deployment
------------------------------------------
From Your NKP Manage-Cluster (NOT Kommander Cluster) Dasboard - Projects - Create Project
Name              :  arief-project
Select Your Managed-Cluster
Create - Continue to project
Go to Continues Deployment (CD) Tab - Add GitOps Source
Name              :  online-boutique
Repository URL    :  https://github.com/nutanixdev/nkp-microservices-demo.git
Git Ref Type      :  Branch
Branch Name       :  main
Path              :  ./release/without-istio
Git Secret        :  none
Save

From Kubernetes Dashboard  -->  Choose Yourname-Project-xxx Namespace - Check Deployment, make sure deployment success (Please be aware that creating new project will create new namespace based on your project name)

Troubleshooting redis card pod (if its happened)
------------------------------------------------

  - redis-cart error (this is happened because the image is in docker hub, while other deployment's image use gcp artifact with no auth and limit -- while docker hub has maximum limit):

From Code-Server (Bastion VM):
  - kubectx --> choose commander context
  
    export HARBOR_ADDRESS="$(kubectl -n kommander get kommandercluster host-cluster -o jsonpath='{.status.ingress.address}')"
    echo $HARBOR_ADDRESS

Create harbor-registry-credentials manifest in new project namespace:
---------------------------------------------------------------------

kubectx --> choose your 1st Managed Cluster (DC)
kubens --> make sure you choose your new project namespace

cat /etc/docker/certs.d/$HARBOR_ADDRESS:5000/ca.crt --> make sure the file exist

REGISTRY_USERNAME="nutanix"
REGISTRY_PASSWORD="nx2Tech974!" #change password to your cluster password

kubectl create secret generic harbor-registry-credentials \
--from-literal username=$REGISTRY_USERNAME \
--from-literal password=$REGISTRY_PASSWORD \
--from-file=/etc/docker/certs.d/$HARBOR_ADDRESS:5000/ca.crt

kubectl get secrets --> "harbor-registry-credentials" secret created

Modify RedisCart Deployment to fix the problem:
-----------------------------------------------
From NKP Dashboard - your workspace - Projects - your project - Continues Deployment (CD) :
  - Click 3 button menu from online-boutique GitOps Source
  - Click Suspend

From Kubernetes Dashboard:
  - Modify Redist-Card Deployment:
      * change the image location to --> image: 10.38.106.41:5000/proxy_cache/redis:alpine #Change the IP Address to your harbor IP

add below config (After dnspolicy): *** MAKE SURE imagePullSecrets IN THE SAME INDENT WITH dnspolicy ***
      imagePullSecrets:
        - name: harbor-registry-credentials

Click Update Button

* Make sure deployment Running (Click Restart from 3 menu button on redis-cart deployment)
* For curousity open your harbor dashboard - Projects - proxy_cache ---> there is redis there in the repo with 1 pull

Go to Services - frontend-external --> click the external end points IP Address (make sure application shown)

From NKP Dashboard - your workspace - Projects - your project - Continues Deployment (CD) :
  - Click 3 button menu from online-boutique GitOps Source
  - Click Resume --> This is necessary if you already fixed the problem in the source code

  but for our case (its deliberetly to be broken), we will make the status SUSPENDED


OPTIONAL LAB #1 (ONLY CAN BE DONE IF YOU HAVE NUTANIX FILES CONFIGURED IN YOUR CLUSTER)
====================================================================================

Create an NFS File Share
------------------------
- From your Prism Central (PC) dasbboard - Unified Storage - Files
- Click your File Server (ex: labFS) - Share & Exports - Click New Share or Export
  * Name                    : wordpress
  * Size                    : 10 GB
  * Primary Protocol Access : NFS
  * General Settings        : Default (no change)
  * Next
  * NFS Protocol Access     : Default (no change)
  * Next
  * Create
- Click your new wordpress share
  * Copy the Mount Path (ex: labFS.ntnxlab.local:/wordpress)

- From Code Proxy (bastion)
  * sudo mkdir /mnt/nfs_wordpress
  * sudo mount -t nfs labFS.ntnxlab.local:/wordpress /mnt/nfs_wordpress
  * cd /mnt/nfs_wordpress

Modify Wordpress Pods into 2 replicas with single PVC Claim
-----------------------------------------------------------
- From kubernetes dashboard - Your Name-nginx name space - Config and Storage 
  * From - Storage Classes --> You will see only 1 named nutanix-volume
  * From - Persistent Volume Claim --> You will see 2 PVC with ReadWriteOnce in nutanix-volume storage class
  * Create new resource - create from input:

    ## change "arief" with your name instead ##

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: arief-nutanix-files
provisioner: csi.nutanix.com
parameters:
  nfsServerName: labFS
  nfsServer: labFS.ntnxlab.local
  nfsPath: wordpress
  storageType: NutanixFiles
  squashType: root-squash
reclaimPolicy: Delete
volumeBindingMode: Immediate
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: arief-wp-pv-claim-nfs
  labels:
    app: wordpress
spec:
  storageClassName: arief-nutanix-files
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi

- From kubernetes dashboard - Your Name-nginx name space
  * From Config and Storage - Storage Classes --> You will see [yourname]-nutanix-files
  * From Persistent Volume Claim --> [yourname]-wp-pv-claim-nfs with Read-Write-Many to nutanix files storage class
  * From Cluster - Persistent Volume --> the PV-xxxxxx-xxxxxx (with storage class nutanix-files) has BOND status

- From kubernetes dashboard - Your Name-nginx name space
  * Modify wordpress deployment:
    - Go to Volumes (line 176 approx):
        - name: [yourname]-wordpress-persistent-storage
          persistentVolumeClaim:
            claimName: [yourname]-wp-pv-claim --> replace '[yourname]-wp-pv-claim' to '[yourname]-wp-pv-claim-nfs'
    - Go to spec (line 130 approx):
        - replicas: 1 --> change it into replicas: 2 (move your cursor UP NOT DOWN)
    - CLick UPDATE
  * Go to Pods:
    - you will see 2 new created PODS 
      - Click 3 dot menu pod no #1 - click exec --> you are now in /var/www/html folder (this is the folder that using NFS CSI share), examine the files
      - Click 3 dot menu pod no #2 - click exec --> you will see same files (use ls)
  * Go to code-server (bastion)
    - ls /mnt/nfs_wordpress --> you will see pvc folder created by CSI driver
    - ls /mnt/nfs_wordpress/pvc-xxxxx --> you will see the same files as pod no #1 and no #2


OPTIONAL LAB #2 (ONLY CAN BE DONE IF YOU HAVE NUTANIX OBJECT CONFIGURED IN YOUR CLUSTER)
========================================================================================

Make sure you have Object Store Configured
------------------------------------------
- From your Prism Central (PC) dasbboard - Unified Storage - Object - Object Stores (Ex: ntnxlab)
- Click your Object Store (ex: ntnxlab) - Summary 
  * Public IPs  : take a note on this IP (ex: 10.38.106.8)
- From the left pane meu - click Access Keys
  * Click Add People
  * Search people or group : Administrator@ntnxlab.local - Click Next
  * Click Generate Key - Download Key

    - Username          : administrator
    - Access Key        : G-v-kw8T3sMqQlEDL_C0-lWNLqpGAY7D
    - Secret Key        : UAAnP7XXRp2AWRDnNg8c3KPQYpc_W8zs
    - Display Name      : Administrator
    - Tag               : buckets-access-key-fSbNGuEhRqXcndXexVUQwBnT

Install Nutanix COSI Driver in workspace
------------------------------------------
- From your NKP Dashboard - Your Workspace - Clusters - Your Managed-Cluster - Application Tab
  * Change category to General
  * Click 3 doted menu of COSI Driver for Nutanix - Enable:
    - Prism Central Endpoint          :   Your PC IP Address (ex: https://10.38.106.7:9440)
    - Prism Central Username          :   admin
    - Prism Central Password          :   Your HPOC cluster password (ex: nx2Tech974!)
    - Nutanix Object Storage Endpoint :   Your Object Public IP Above (ex: http://10.38.106.8) 
    - Nutanix Object Access Key       :   Your Access Key Above (ex: G-v-kw8T3sMqQlEDL_C0-lWNLqpGAY7D)
    - Nutanix Object Secret Key       :   Your Secret Key Above (ex: UAAnP7XXRp2AWRDnNg8c3KPQYpc_W8zs)
    - Save
    - Wait until Nutanix COSI Driver "Deployed" not "Enable"

Check Nutanix COSI Driver Components:
-------------------------------------
- From your Code-Server (Bastion VM):
  * kubectl get pods -n container-object-storage-system (ex:container-object-storage-controller-6f7856b456-468xm running status )
  * kubectl get secrets cosi-driver-nutanix -n arief-workspace-f568l-hlfzt  (change to your namepace)
  * kubectl get crd | grep objectstorage.k8s.io
        bucketclasses.objectstorage.k8s.io                         2025-08-01T23:21:52Z
        bucketclaims.objectstorage.k8s.io                          2025-08-01T23:21:51Z
        bucketaccessclasses.objectstorage.k8s.io                   2025-08-01T23:21:51Z
        bucketaccesses.objectstorage.k8s.io                        2025-08-01T23:21:51Z
        buckets.objectstorage.k8s.io                               2025-08-01T23:21:52Z

Membuat Bucket di Object Storage Melalui Yaml:
----------------------------------------------
- From your kubernetes dashboard - your name namespace - nginx (ex: arief-nginx)
  * Create new resource - create from input:

apiVersion: objectstorage.k8s.io/v1alpha1
kind: BucketClass
metadata:
  name: cosi-bucketclass
driverName: ntnx.objectstorage.k8s.io
deletionPolicy: Delete
parameters:
  provisioner_data: "test_data"

   * Create new resource - create from input:

apiVersion: objectstorage.k8s.io/v1alpha1
kind: BucketClaim
metadata:
  name: cosi-bucketclaim
spec:
  bucketClassName: cosi-bucketclass
  protocols:
    - s3

  *** Check your bucket has been created ***
        - From Code-Server (Bastion VM):
          * kubectl -n arief-nginx get bucketclaim  --> bucket shown
            Ex: 
            
            NAME               AGE
            cosi-bucketclaim   4h58m

        - From your Prism Central (PC) dasbboard - Unified Storage - Object - Object Stores (Ex: ntnxlab)
        - Click your Object Store (ex: ntnxlab) - Buckets
          * You will find new bucket create named cosi-bucketclassxxxxxx
          * Click "Launch Objects Browser"
          * input your Access and Secret Keys and Login
          * You will see your bucket has been created
          * Click your cosi-bucketclass3f1a3aef-f215-4b47-afa3-bbf68dbaf70e --> you will see "no objects yet"

   * Create new resource - create from input:

apiVersion: objectstorage.k8s.io/v1alpha1
kind: BucketAccessClass
metadata:
  name: cosi-bucketaccessclass
driverName: ntnx.objectstorage.k8s.io
authenticationType: KEY
parameters:
  provisioner_data: "test_data"

   * Create new resource - create from input:

apiVersion: objectstorage.k8s.io/v1alpha1
kind: BucketAccess
metadata:
  name: cosi-bucketaccess
spec:
  bucketClaimName: cosi-bucketclaim
  protocol: s3
  bucketAccessClassName: cosi-bucketaccessclass
  credentialsSecretName: bucketcreds

  *** From your kubernetes dashboard - Go to Config and Storage - Secret --> you will find "bucketcreds" secret created ***
      - From Code-Server (Bastion VM):
        * kubectl -n arief-nginx get secret bucketcreds -o jsonpath='{.data.BucketInfo}' | base64 -d | jq
          Ex:
                    {
                      "metadata": {
                        "name": "bc-63df068d-8510-44cf-9a01-7f0d1e4d4018",
                        "creationTimestamp": null
                      },
                      "spec": {
                        "bucketName": "cosi-bucketclassf5e2b599-7b6f-43cd-935f-612265cdf0a8",
                        "authenticationType": "KEY",
                        "secretS3": {
                          "endpoint": "http://10.38.106.8:80",
                          "region": "us-east-1",
                          "accessKeyID": "B-pTgRe-FbhPmqm850IcCG8Qq7G6CMmV",
                          "accessSecretKey": "8v98uSISYb6jN-PhVbh95xLBHT-uUwc5"
                        },
                        "secretAzure": null,
                        "protocols": [
                          "s3"
                        ]
                      }
                    }

  *** Run this procedure from AUTOAD VM ***
      - From your Prism Central (PC) dasbboard - Unified Storage - Object - Object Stores (Ex: ntnxlab)
        - Click your Object Store (ex: ntnxlab) - Summary 
        - From the left pane meu - click Access Keys
            * You will find new access key created by cosi-driver named your-workspace-xxxxx 
                * you will see sames access key as secret in bucketcreds above:
                  Ex: Access Key : _4L2QdjWzth2o4wOj3fNslnJP9Z4FGEw
        - From the left pane menu - click Object Stores - click ntnxlab object store - buckets tab
            * Click Launch Objects Browser
                - Access Key      : AccessKeyID stated above (ex: _4L2QdjWzth2o4wOj3fNslnJP9Z4FGEw )
                - AccessSecretKey : AccessSecretKey stated above (ex: O7U-qMMWlFvti8T8_5ftqALeB9tbim25 )
                - Login
                - Click cosi-bucketclassxxxxx
                - Upload Objects Select Files
                - Choose nexus-3.82.0-08-linux-x86_64.tar.gz from c:\inetpub\wwwroot
                - Until status uploaded
                - Click Close
            * Take note on the bucket name (ex: cosi-bucketclass86f97cef-df9f-4eae-97de-459cdbf01781)

Try COSI Driver from POD
-------------------------
- From your kubernetes dashboard - your name namespace - nginx (ex: arief-nginx)
  * Create new resource - create from input:


*** THIS PROCEDURE WILL CREATE A ROLE AND ROLEBINDING TO DEFAULT SERVICE ACCOUNT SO IT CAN READ SECRETS ***

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: secret-reader
  namespace: arief-nginx
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "list"]

---

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: secret-reader-binding
  namespace: arief-nginx
subjects:
- kind: ServiceAccount
  name: default
  namespace: arief-nginx
roleRef:
  kind: Role
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io


*** THIS PROCEDURE WILL CREATE A POD WITH INITCONTAINER (FOR EXTRACTING BUCKETCREDS TO FILE AWS-CRED.SH) AND MAIN CONTAINER (FOR WRITE TILE TO BUCKET) ***

  * Create new resource - create from input: *** PUT YOUR BUCKET INFORMATION BELOW ***

apiVersion: v1
kind: Pod
metadata:
  name: s3-client
  namespace: arief-nginx
spec:
  volumes:
    - name: env-volume
      emptyDir: {}

  initContainers:
    - name: init-decode-secret
      image: 10.38.106.41:5000/proxy_cache/bitnami/kubectl:latest    # Change the IP into your harbor proxy_cache URL
      command:
        - /bin/sh
        - -c
        - |
          echo "Getting secret and decoding..."
          SECRET=$(kubectl get secret bucketcreds -n arief-nginx -o jsonpath='{.data.BucketInfo}' | base64 -d)
          echo "Exporting to env file..."
          echo "export AWS_ACCESS_KEY_ID=$(echo $SECRET | jq -r '.spec.secretS3.accessKeyID')" > /env/aws-creds.sh
          echo "export AWS_SECRET_ACCESS_KEY=$(echo $SECRET | jq -r '.spec.secretS3.accessSecretKey')" >> /env/aws-creds.sh
          echo "export AWS_REGION=$(echo $SECRET | jq -r '.spec.secretS3.region')" >> /env/aws-creds.sh
          echo "export AWS_ENDPOINT=$(echo $SECRET | jq -r '.spec.secretS3.endpoint')" >> /env/aws-creds.sh
          cat /env/aws-creds.sh
      volumeMounts:
        - name: env-volume
          mountPath: /env
  imagePullSecrets:
  - name: harbor-registry-credentials

  containers:
    - name: main
      image: 10.38.106.41:5000/proxy_cache/amazon/aws-cli            # Change the IP into your harbor proxy_cache URL
      command:
        - /bin/sh
        - -c
        - |
          echo "Starting main container..."
          source /env/aws-creds.sh

          # wait until bucket exists
          export BUCKET_NAME=cosi-bucketclass86f97cef-df9f-4eae-97de-459cdbf01781 # PUT YOUR BUCKET NAME HERE  
          echo "BUCKET_NAME=cosi-bucketclass86f97cef-df9f-4eae-97de-459cdbf01781" >> /env/aws-creds.sh # PUT YOUR BUCKET NAME HERE 
          source /env/aws-creds.sh
          echo "Waiting for bucket $BUCKET_NAME to be available..."
          while ! aws --endpoint-url=$AWS_ENDPOINT s3 ls s3://$BUCKET_NAME/ >/dev/null 2>&1; do
            echo "Bucket not ready yet... retrying in 5 seconds"
            sleep 5
          done
          echo "Bucket is now available."

          echo 'File ini dibuat oleh pod melalui cosi driver' > pod-created-file.txt
          echo 'Uploading file to bucket...'
          aws --endpoint-url=$AWS_ENDPOINT s3 cp pod-created-file.txt s3://$BUCKET_NAME/
          echo "Listing S3 bucket using AWS CLI..."
          aws --endpoint-url=$AWS_ENDPOINT s3 ls
          echo "Listing Files Inside the Bucket ..."
          aws --endpoint-url=$AWS_ENDPOINT s3 ls s3://$BUCKET_NAME/
          sleep 3600
      volumeMounts:
        - name: env-volume
          mountPath: /env
  imagePullSecrets:
  - name: harbor-registry-credentials

*** CHECK BUCKET ACCESS FROM INSIDE THE CONTAINER ***

  - Go to Pods on the left pane menu in your kubernetes dashboard
    * You will find s3-client PODS created. (wait until it status is RUNNING)
    * Click 3 dot menu on the pod and click logs --> examine the init pods and main pod logs
        ** In Init pod Logs --> You will see extracted bucketcreds secret
        ** In Main Pod Logs --> You will see you will see file uploaded and list of existing files (including file you have uploaded) in the bucket

  - From your Prism Central (PC) dasbboard - Unified Storage - Object - Object Stores (Ex: ntnxlab)
      - Click your Object Store (ex: ntnxlab) - Buckets
          * You will find new bucket create named cosi-bucketclassxxxxxx
          * Click "Launch Objects Browser"
          * input your Access and Secret Keys and Login
          * Click your cosi-bucketclass3f1a3aef-f215-4b47-afa3-bbf68dbaf70e
          * You will find new file "pod-created-file.txt" created by the script from main container using cosi-driver

*** IF YOU NEED TO RECREATE THE WHOLE COSI LABS - PLEASE CLEAN THE MANIFEST USING BELOW SCRIPTS ***

From Code-Server (Bastion VM)
-----------------------------

kubens # Choose your name-nginx namespace
*** list ***
kubectl get roles secret-reader
kubectl get RoleBinding secret-reader-binding
kubectl get bucketclass cosi-bucketclass
kubectl get bucketclaim cosi-bucketclaim
kubectl get bucketaccessclass cosi-bucketaccessclass
kubectl get bucketaccess cosi-bucketaccess
kubectl get secret bucketcreds
kubectl get bucket
kubectl get pods s3-client

*** Delete ***
kubectl get bucket --> Take note of the bucket name (Ex: cosi-bucketclass86f97cef-df9f-4eae-97de-459cdbf01781)
kubectl patch bucket cosi-bucketclass86f97cef-df9f-4eae-97de-459cdbf01781 -p '{"metadata":{"finalizers":[]}}' --type=merge
kubectl delete bucket cosi-bucketclass86f97cef-df9f-4eae-97de-459cdbf01781 

  - From your Prism Central (PC) dasbboard - Unified Storage - Object - Object Stores (Ex: ntnxlab)
      - Click your Object Store (ex: ntnxlab) - Buckets
          * Select your bucket (ex: cosi-bucketclass86f97cef-df9f-4eae-97de-459cdbf01781)
          * Action - Delete
          * OK

kubectl delete roles secret-reader 
kubectl delete RoleBinding secret-reader-binding
kubectl patch bucketclass cosi-bucketclass -p '{"metadata":{"finalizers":[]}}' --type=merge
kubectl delete bucketclass cosi-bucketclass
kubectl patch bucketclaim cosi-bucketclaim -p '{"metadata":{"finalizers":[]}}' --type=merge
kubectl delete bucketclaim cosi-bucketclaim
kubectl patch bucketaccessclass cosi-bucketaccessclass -p '{"metadata":{"finalizers":[]}}' --type=merge
kubectl delete bucketaccessclass cosi-bucketaccessclass
kubectl patch bucketaccess cosi-bucketaccess -p '{"metadata":{"finalizers":[]}}' --type=merge
kubectl delete bucketaccess cosi-bucketaccess
kubectl patch secret bucketcreds -p '{"metadata":{"finalizers":[]}}' --type=merge
kubectl delete secret bucketcreds
kubectl delete pods s3-client --force

You can now re-start the COSI lab procedures.






