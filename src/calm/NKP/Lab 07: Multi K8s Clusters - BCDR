MULTI KUBERNETES CLUSTER ENVIRONMENTS
-------------------------------------

*** A. BACKUP RESTORE BETWEEN 2 CLUSTERS WITH VELERO ***
--------------------------------------------------------

*** Create 2nd Managed Kubernetes Cluster ***

Secondary vlan IP range     :   x.x.x.132-254   --> By default secondary IP is still FREE
Secondary IPAM              :   x.x.x.161-253

Using NKP Dashboard - Your Name Workspace - Select Clusters from left pane menu
    -   Click Add Cluster - Create Cluster
        -   Cluster Name    		:   yourname-nkp-drc (Ex: arief-nkp-drc)
        ⁃	SSH Public Key			:	ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBM2C9FLMlL6xXcMRZt6Yjf1cgadN/1S6M8V3M6OQDbD arief.pribadi@nutanix.com # Check cat .ssh/id_ed25519.pub in bastion VM
		⁃	Nutanix Prism Project	:	lab
		⁃	Nutanix AOS Cluster		:	PHX-POC334
		⁃	Subnet			        :	aux-1 --> Your secondary Subnet
		⁃	OS Image				:	nkp-rocky-9.5-release-1.31.9-20250702204537.qcow2
		⁃	CP End Point IP			:	1st IP from secondary vlan (132) --> make sure checked by PING or IP Scanner its a FREE IP (ex: 10.38.106.132)
		⁃	CP Node Count			:	1
		⁃	CPU Per Node (vCPU)		:	4
		⁃	Memory Per Node			:	12
		⁃	Disk Size Per Node		:	80
		⁃	Nutanix Prism Project	:	lab
        ⁃	Nutanix AOS Cluster		:	PHX-POC334
		⁃	Subnet					:	aux-1 --> Your secondary Subnet
		⁃	OS Image				:	nkp-rocky-9.5-release-1.31.9-20250702204537.qcow2
		⁃	Worker Node Count		:	4
		-	CPU Per Node (vCPU)		:	8
		⁃	Memory Per Node			:	12
		⁃	Disk Size Per Node		:	80
		⁃	Storage Container			default

        *** PAY ATTENTION TO BELOW CONFIGURATION ***

        -   Pod Network             :   172.20.0.0/16  	(NOT --> 192.168.0.0/16)
        -   Service Network         :   10.96.0.0/12


		⁃	Load Balancing Start	:	x.x.x.140)--> (by default secondary vlan is free) (ex: 10.38.106.140)
		⁃	Load Balancing End		:	x.x.x.150 --> make sure checked by PING or IP Scanner its a FREE IP (ex: 10.38.106.150)
		⁃	Image Registry Mirror	
			⁃	URL					:	https://airgap.arief.com:5000
			⁃	username			:	admin
			⁃	password			:	nutanix/4u
			⁃	CA					:	downloaded cert.crt
		⁃	Private Registry	
			⁃	URL					:	https://10.38.106.41:5000	# Your harbor URL Access as noted above
			⁃	username			:	nutanix
			⁃	password			:	nx2Tech974!
			⁃	CA					:	downloaded ca.cert
	-	Create

Using NKP Dashboard - Your Name Workspace - Select Clusters from left pane menu
    -   Click yourname-nkp-drc cluster - Application
        *   Make sure all the application in starts "Deployed" not "Enable"

	- Generate Token (by clicking your username from the right top corner)
	- Login using above username and password
		* Put the CA Certificate into your .kube directory --> Copy and Paste in code-server terminal (bastion)
		* These commands will update ~/.kube/config --> Copy and Paste in code-server terminal (bastion) 4x times
		* from code-server terminal --> kubectl get nodes
	- Modify your .kube\config 'context name' with more friendly name
		* ex: name: objective_chaplygin-10.38.106.53 (Managed Cluster) --> for managed cluster
		* ex: name: objective_chaplygin-10.38.106.41 (Kommander) --> for commander cluster
        * ex: name: objective_chaplygin-10.38.106.32 (Managed DRC) --> for managed DRC cluster
	- use kubectx to switch between cluster-context

*** Other way to manage multi cluster is to merge --flaten all kubeconfig file ***

    -   Download all kubeconfig file into bastion vm (you already have kommander kubeconfig file in bastio)
    -   Merge all 3 kubeconfig --> pay attention on how kubeconfig file use user certificate for authentication rather than TOKEN

        KUBECONFIG=airgapped-hm-nkp-demo-2.conf:arief-nkp-drc-kubeconfig.yaml:arief-nkp-kubeconfig.yaml kubectl config view --flatten > ~/.kube/config

*** Install Velero on both Managed Cluster ***

1.  From NKP Dashboard - Your Name Workspace - Application - Set Categories to Backup
    -   Enable Rook Ceph
    -   Check your 2 managed clusters
    -   Click Enable
    -   Verify in both clusters that velero has been "Deployed" not only "Enable"
    -   Enable Rook Ceph Cluster
    -   Check your 2 managed clusters
    -   Click Enable
    -   Verify in both clusters that velero has been "Deployed" not only "Enable"
    -   Enable Velero
    -   Check your 2 managed clusters
    -   Click Enable
    -   Verify in both clusters that velero has been "Deployed" not only "Enable"

2.  Install Velero CLI on bastion VM
    - From Code-server (bastion mv)
        *   curl -L https://github.com/vmware-tanzu/velero/releases/download/v1.15.2/velero-v1.15.2-linux-amd64.tar.gz -o velero.tar.gz
        *   tar -xvf velero.tar.gz
        *   sudo mv velero-v1.15.2-linux-amd64/velero /usr/local/bin/
        *   velero version -n kommander

3.  From Prism Central (PC) - Unified Storage - Objects
    -   Take a note on Object Store ntnxlab Object public IP --> ex: 10.38.106.8
    -   Click ntnxlab Object Stores
    -   Click Buckets Tab - Create Bucket
        * Name      :   velero-backup   #   Take note on bucket name
        * Create
    -   From bucket tab
        * click velero-backup - User Access - Edit User Access - Add Users and Permissions
        * Add Administrator with FULL ACCESS
        * Save
    -   Click Access Key
        * Choose Administrator - Manage
        * Add Key - Generate Key
        * Download Key

            Username: administrator
            Access Key: kzj-yB1gXXG1h0_57-15JGVQx4pd29Mo
            Secret Key: ytexinDnjN2kK1TGvNMWH4Al2ykmo6VV
            Display Name: Administrator
            Tag: buckets-access-key-XSwkYvRMQCVvpOGYkeVrZbdr

    *** Check Bucket Connectivity ***

        - Install AWS Cclient   
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip awscliv2.zip
            sudo ./aws/install

        - Create Temporary Credential

mkdir ~/.aws

cat > ~/.aws/credentials <<EOF
[ntnx-object-nkp]
aws_access_key_id = kzj-yB1gXXG1h0_57-15JGVQx4pd29Mo                            #   YOUR_KEY_ID
aws_secret_access_key =  ytexinDnjN2kK1TGvNMWH4Al2ykmo6VV                       #   YOUR_SECRET_KEY
EOF

cat > ~/.aws/config <<EOF
[profile ntnx-object-nkp]
region = us-east-1
s3 =
endpoint_url = http://10.38.106.8:80
addressing_style = path
EOF

        - Check list bucket

AWS_PROFILE=ntnx-object-nkp aws s3 ls --endpoint-url http://10.38.106.8:80

output ex:  2025-08-03 12:54:22 velero-backup

AWS_PROFILE=ntnx-object-nkp aws s3 ls s3://velero-backup --endpoint-url http://10.38.106.8:80

output  ex: nothing --> no files yet

4.  From code-server (bastion vm)
    -   kubectx --> to your kommander
    -   kubectl get kommandercluster -A  --> take note on your clusters name (ex: as below)
        
        NAMESPACE                     NAME            DISPLAY NAME   STATUS   KUBEFED CLUSTER   AGE
        arief-workspace-f568l-hlfzt   arief-nkp                      Joined   arief-nkp         2d23h
        arief-workspace-f568l-hlfzt   arief-nkp-drc                  Joined   arief-nkp-drc     4h4m
        kommander                     host-cluster                   Joined   host-cluster      3d23h

    -   kubectl get crd | grep velero --> valero.io CRD (Custom Resource Definition) listed

5.  Confirm Velero Installation on Managed-Cluster DC and Backup Yourname-NGINX Namespace

    -   From code-server (bastion vm)

        export CLUSTER_NAME=arief-nkp                                   # Your 1st Managed Cluster (DC) Name in No. 4 Above
        export BUCKET=velero-backup                                     # Your Bucket Name in No. 3 Above
        export WORKSPACE_NAMESPACE=arief-workspace-f568l-hlfzt          # Your 1st Managed Cluster (DC) Namepace in No. 4 Above
        export BSL_NAME=ntnx-object-nkp 
        export NUTANIX_OBJECTS_HOST=10.38.106.8                         # Your Object Store IP Address in No. 3 Above
        export NUTANIX_OBJECTS_PORT=80
        export NUTANIX_OBJECTS_SECRET=velero-nutanix-credentials
        export AWS_PROFILE=ntnx-object-nkp
        export NUTANIX_OBJECTS_ACCESS_KEY_ID=kzj-yB1gXXG1h0_57-15JGVQx4pd29Mo           # Your Object Store AccessKey in No. 3 Above
        export NUTANIX_OBJECTS_SECRET_ACCESS_KEY=ytexinDnjN2kK1TGvNMWH4Al2ykmo6VV       # Your Object Store SecretKey in No. 3 Above

6.  From code-server (bastion vm)

*** Create Secret to Store Bucket Credential in Managed-Cluster DC Namespace ***

kubectx --> choose your 1st managed cluster (dc) cluster

    -   kubectl get pod -A | grep velero  # --> check velero installation

arief-workspace-f568l-hlfzt       object-bucket-claims-check-dkp-velero-cd5qk                       0/1     Completed   0                5h9m
arief-workspace-f568l-hlfzt       velero-6c94655cb9-c8hkn                                           1/1     Running     0                5h9m
arief-workspace-f568l-hlfzt       velero-backup-storage-location-updater-77446646cd-wzlmv           1/1     Running     0                5h8m
arief-workspace-f568l-hlfzt       velero-pre-install-jt4wm                                          0/1     Completed   0                5h40m

kubectl apply -f - <<EOF 
apiVersion: v1
kind: Secret
metadata:
  name: ${NUTANIX_OBJECTS_SECRET}
  namespace: ${WORKSPACE_NAMESPACE}  
type: Opaque
stringData:
  aws: |
    [${AWS_PROFILE}]
    aws_access_key_id=${NUTANIX_OBJECTS_ACCESS_KEY_ID}
    aws_secret_access_key=${NUTANIX_OBJECTS_SECRET_ACCESS_KEY}
EOF

kubectl get secret ${NUTANIX_OBJECTS_SECRET} -n ${WORKSPACE_NAMESPACE} -o jsonpath="{.data.aws}" | base64 --decode

Ex:

[ntnx-object-nkp]
aws_access_key_id=kzj-yB1gXXG1h0_57-15JGVQx4pd29Mo
aws_secret_access_key=ytexinDnjN2kK1TGvNMWH4Al2ykmo6VV

*** Create ConfigMap to Create backupStorageLocation (BSL) of Velero in Managed-Cluster DC Namespace ***

kubectx --> choose your 1st managed cluster (dc) cluster


velero backup-location create ${BSL_NAME} -n ${WORKSPACE_NAMESPACE} --provider aws --bucket ${BUCKET} --credential ${NUTANIX_OBJECTS_SECRET}=aws --config region=us-east-1,insecureSkipTLSVerify="true",s3ForcePathStyle="true",profile=${AWS_PROFILE},s3Url=http://${NUTANIX_OBJECTS_HOST}:${NUTANIX_OBJECTS_PORT}

kubectl get bsl -n ${WORKSPACE_NAMESPACE}

Output Ex:

NAME              PHASE       LAST VALIDATED   AGE     DEFAULT
default           Available   37s              3h34m   
ntnx-object-nkp   Available   28s              28s     

kubectl get backupstoragelocations -n ${WORKSPACE_NAMESPACE} -oyaml # --> to troubleshoot if the status NOT "available".

7.  Backup Process

*** Test backup in Managed-Cluster DC Namespace ***

velero backup create nutanix-velero-testbackup -n ${WORKSPACE_NAMESPACE} --storage-location ntnx-object-nkp --snapshot-volumes=false
velero backup describe nutanix-velero-testbackup -n ${WORKSPACE_NAMESPACE}
AWS_PROFILE=ntnx-object-nkp aws s3 ls s3://velero-backup/backups/ --endpoint-url http://10.38.106.8:80

    --> nutanix-velero-testbackup shown

Check Bucket from Prism Central:

  - From your Prism Central (PC) dasbboard - Unified Storage - Object - Object Stores (Ex: ntnxlab)
      - Click your Object Store (ex: ntnxlab) - Buckets
          * Click "Launch Objects Browser"
          * input your Access and Secret Keys and Login
          * Click your velero-backup
          * You will find new backup folder - click backup folder
          * You will see your backup job folder (nutanix-velero-testbackup) - click it
          * You will see all arficact backup


*** Backup Production yourname-nginx namespace in Managed-Cluster DC ***

velero backup create arief-nginx-backup -n ${WORKSPACE_NAMESPACE} --storage-location ntnx-object-nkp --snapshot-volumes=true --include-namespaces arief-nginx

velero backup describe arief-nginx-backup -n ${WORKSPACE_NAMESPACE}

AWS_PROFILE=ntnx-object-nkp aws s3 ls s3://velero-backup/backups/ --endpoint-url http://10.38.106.8:80

    --> nutanix-velero-testbackup shown

Check Bucket from Prism Central:

  - From your Prism Central (PC) dasbboard - Unified Storage - Object - Object Stores (Ex: ntnxlab)
      - Click your Object Store (ex: ntnxlab) - Buckets
          * Click "Launch Objects Browser"
          * input your Access and Secret Keys and Login
          * Click your velero-backup
          * You will find new backup folder - click backup folder
          * You will see your backup job folder (nutanix-velero-testbackup) - click it
          * You will see all arficact backup

8.  Restore Process

*** Restore yourname-nginx namespace in Managed-Cluster DRC ***

kubectx --> choose your 2nd managed cluster (drc) cluster

    -   From code-server (bastion vm)

        export CLUSTER_NAME=arief-nkp-drc                               # Your 2nd Managed Cluster (DRC) Name in No. 4 Above
        export BUCKET=velero-backup                                     # Your Bucket Name in No. 3 Above
        export WORKSPACE_NAMESPACE=arief-workspace-f568l-hlfzt          # Your 1st Managed Cluster (DC) Namepace in No. 4 Above
        export BSL_NAME=ntnx-object-nkp 
        export NUTANIX_OBJECTS_HOST=10.38.106.8                         # Your Object Store IP Address in No. 3 Above
        export NUTANIX_OBJECTS_PORT=80
        export NUTANIX_OBJECTS_SECRET=velero-nutanix-credentials
        export AWS_PROFILE=ntnx-object-nkp
        export NUTANIX_OBJECTS_ACCESS_KEY_ID=kzj-yB1gXXG1h0_57-15JGVQx4pd29Mo           # Your Object Store AccessKey in No. 3 Above
        export NUTANIX_OBJECTS_SECRET_ACCESS_KEY=ytexinDnjN2kK1TGvNMWH4Al2ykmo6VV       # Your Object Store SecretKey in No. 3 Above
    
*** Create Secret to Store Bucket Credential in Managed-Cluster DRC  Namespace ***

kubectx --> choose your 2nd managed cluster (drc) cluster

    -   kubectl get pod -A | grep velero  # --> check velero installation

arief-workspace-f568l-hlfzt       object-bucket-claims-check-dkp-velero-cd5qk                       0/1     Completed   0                5h9m
arief-workspace-f568l-hlfzt       velero-6c94655cb9-c8hkn                                           1/1     Running     0                5h9m
arief-workspace-f568l-hlfzt       velero-backup-storage-location-updater-77446646cd-wzlmv           1/1     Running     0                5h8m
arief-workspace-f568l-hlfzt       velero-pre-install-jt4wm                                          0/1     Completed   0                5h40m

kubectl apply -f - <<EOF 
apiVersion: v1
kind: Secret
metadata:
  name: ${NUTANIX_OBJECTS_SECRET}
  namespace: ${WORKSPACE_NAMESPACE}  
type: Opaque
stringData:
  aws: |
    [${AWS_PROFILE}]
    aws_access_key_id=${NUTANIX_OBJECTS_ACCESS_KEY_ID}
    aws_secret_access_key=${NUTANIX_OBJECTS_SECRET_ACCESS_KEY}
EOF

kubectl get secret ${NUTANIX_OBJECTS_SECRET} -n ${WORKSPACE_NAMESPACE} -o jsonpath="{.data.aws}" | base64 --decode

Ex:

[ntnx-object-nkp]
aws_access_key_id=kzj-yB1gXXG1h0_57-15JGVQx4pd29Mo
aws_secret_access_key=ytexinDnjN2kK1TGvNMWH4Al2ykmo6VV

*** Create ConfigMap to Create backupStorageLocation (BSL) of Velero in Managed-Cluster DRC Namespace ***

velero backup-location create ${BSL_NAME} -n ${WORKSPACE_NAMESPACE} --provider aws --bucket ${BUCKET} --credential ${NUTANIX_OBJECTS_SECRET}=aws --config region=us-east-1,insecureSkipTLSVerify="true",s3ForcePathStyle="true",profile=${AWS_PROFILE},s3Url=http://${NUTANIX_OBJECTS_HOST}:${NUTANIX_OBJECTS_PORT}

kubectl get bsl -n ${WORKSPACE_NAMESPACE}

kubectl get backupstoragelocations -n ${WORKSPACE_NAMESPACE} -oyaml # --> to troubleshoot if the status NOT "available".

*** Restore yourname-nginx namespace in Managed-Cluster DRC ***

velero restore create arief-nginx-restore -n ${WORKSPACE_NAMESPACE} --from-backup arief-nginx-backup --include-namespaces arief-nginx

velero backup describe arief-nginx-backup -n ${WORKSPACE_NAMESPACE}

*** Check your restore using kubernetes dashboard in Managed-Cluster DRC ***

1.  You will find that your nginx deployment smoothly up and running           --  why?
2.  You will find that your wordpress-mysql smoothly up and running            --   why?
3.  You will find that your wordpress-xxx-xxx pods still in pending status:
    *   0/5 nodes are available: pod has unbound immediate PersistentVolumeClaims. preemption: 0/5 nodes are available: 
    *   Why?

Solution:

- From kubernetes dashboard - Your Name-nginx name space - Config and Storage 
  * From - Storage Classes --> You will see only 1 named nutanix-volume (or with dkp-object-store if exist)
  * From - Persistent Volume Claim --> You will see a lot of PVC with ReadWriteOnce in nutanix-volume storage class but none of its Nutanix File.
  * Create new resource - create from input:

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nutanix-files
provisioner: csi.nutanix.com
parameters:
  nfsServerName: labFS
  nfsServer: labFS.ntnxlab.local
  nfsPath: wordpress
  storageType: NutanixFiles
  squashType: root-squash
reclaimPolicy: Delete
volumeBindingMode: Immediate


*** B. DATA PROTECTION AND REPLICATION BETWEEN 2 CLUSTERS WITH NUTANIX DATA SERVICE FOR KUBERNETES (NDK) ***
------------------------------------------------------------------------------------------------------------

1.  NDK Airgap Installation on Your 1st Managed Cluster (DC)

    -   In AUTOAD VM, open chrome and From the (NDK) page (https://portal.nutanix.com/page/downloads?product=ndk), download the following:
            *   NDK Airgap Bundle containing the container images:
                -   Nutanix Data Services for Kubernetes ( Version: 1.2.0 )

    -   Using WinSCP copy ALL of your files to Bastion VM /home/nutanix (ex: ndk.1.2.0.tar)

    -   Using Code-Server (Bastion VM)
            *   tar -xvf ndk-1.2.0.tar
            *   Modify 1 wrong yaml file   
                    vi ndk-1.2.0/chart/templates/crd-upgrade-hook.yaml 

                    Replace this line 17:

                        data:
                          {{ $base }}.yaml: |
                        {{ $fileContents | toYaml | indent 4 }}
                        ---
                        {{- end }}
                        apiVersion: batch/v1

                    With this :

                        data:
                         {{ $base }}.yaml: |
                        {{ $fileContents | toYaml | indent 4 }}
                        {{- end }}
                        ---
                        apiVersion: batch/v1

        *** Above fixing by moving '---' UNDER '{{- end }} NOT UPPER ***
        
            *   kubectx --> point to your kommander cluster
            *   echo "$(kubectl -n kommander get kommandercluster host-cluster -o jsonpath='{.status.ingress.address}'):5000"
                -   take not on your harbor URL (ex: 10.38.106.41:5000 )
            *   docker image load -i ndk-1.2.0/ndk-1.2.0.tar
            *   docker image ls
            *   docker image tag ndk/manager:1.2.0 10.38.106.41:5000/library/ndk/manager:1.2.0      # --> use your airgap URL
            *   docker image ls
            *   docker push 10.38.106.41:5000/library/ndk/manager:1.2.0
            *   Lets tag and push the rest of images:
                
                docker image ls --> take not on TAG version on each image and put it to correct image and version below:

                docker image tag ndk/infra-manager:1.2.0 10.38.106.41:5000/library/ndk/infra-manager:1.2.0
                docker image tag ndk/job-scheduler:1.2.0 10.38.106.41:5000/library/ndk/job-scheduler:1.2.0
                docker image tag ndk/kube-rbac-proxy:v0.19.0 10.38.106.41:5000/library/ndk/kube-rbac-proxy:v0.19.0
                docker image tag ndk/kubectl:1.32.3 10.38.106.41:5000/library/ndk/kubectl:1.32.3

                docker push 10.38.106.41:5000/library/ndk/infra-manager:1.2.0
                docker push 10.38.106.41:5000/library/ndk/job-scheduler:1.2.0
                docker push 10.38.106.41:5000/library/ndk/kube-rbac-proxy:v0.19.0
                docker push 10.38.106.41:5000/library/ndk/kubectl:1.32.3

                Open your harbor dashboard and make sure all the images are listed.

            *   Install NDK using to 1st managed cluster (DC) and ntnx-system namespace
                -   kubectx --> make sure you point to your 1st managed cluster (DC)
                -   Create secret in ntnx-system namespace
                        REGISTRY_USERNAME="nutanix"
                        REGISTRY_PASSWORD="nx2Tech974!"     #   --> put your HPOC cluster password

                        kubectl create secret generic -n ntnx-system harbor-registry-credentials \
                        --from-literal username=$REGISTRY_USERNAME \
                        --from-literal password=$REGISTRY_PASSWORD \
                        --from-file=/etc/docker/certs.d/10.38.106.41:5000/ca.crt
                
                        kubectl create secret generic ntnx-pc-secret \
                        --from-literal=key='10.38.106.7:9440:admin:nx2Tech974!' \
                        -n ntnx-system

                -   Execute this command below: --> use your Harbor URL

                helm install ndk -n ntnx-system ndk-1.2.0/chart/ \
                --set manager.repository=10.38.106.41:5000/library/ndk/manager \
                --set manager.tag=1.2.0 \
                --set infraManager.repository=10.38.106.41:5000/library/ndk/infra-manager \
                --set infraManager.tag=1.2.0 \
                --set kubeRbacProxy.repository=10.38.106.41:5000/library/ndk/kube-rbac-proxy \
                --set kubeRbacProxy.tag=v0.19.0 \
                --set kubectl.repository=10.38.106.41:5000/library/ndk/kubectl \
                --set kubectl.tag=1.32.3 \
                --set jobScheduler.repository=10.38.106.41:5000/library/ndk/job-scheduler \
                --set jobScheduler.tag=1.2.0 \
                --set tls.server.enable=false \
                --set config.secret.name=nutanix-csi-credentials \
                --set imageCredentials.imagePullSecretName=harbor-registry-credentials

            *   kubectl get svc ndk-intercom-service -n ntnx-system

                NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)          AGE
                ndk-intercom-service   LoadBalancer   10.103.215.62   10.38.106.56   2021:31993/TCP   114s


2.  NDK Airgap Installation on Your 2nd Managed Cluster (DRC)

            *   Install NDK using to 2nd managed cluster (DRC) and ntnx-system namespace
                -   kubectx --> make sure you point to your 2nd managed cluster (DRC)
                -   Create secret in ntnx-system namespace
                        REGISTRY_USERNAME="nutanix"
                        REGISTRY_PASSWORD="nx2Tech974!"     #   --> put your HPOC cluster password

                        kubectl create secret generic -n ntnx-system harbor-registry-credentials \
                        --from-literal username=$REGISTRY_USERNAME \
                        --from-literal password=$REGISTRY_PASSWORD \
                        --from-file=/etc/docker/certs.d/10.38.106.41:5000/ca.crt
                
                        kubectl create secret generic ntnx-pc-secret \
                        --from-literal=key='10.38.106.7:9440:admin:nx2Tech974!' \
                        -n ntnx-system

                -   Execute this command below: --> use your Harbor URL

                helm install ndk -n ntnx-system ndk-1.2.0/chart/ \
                --set manager.repository=10.38.106.41:5000/library/ndk/manager \
                --set manager.tag=1.2.0 \
                --set infraManager.repository=10.38.106.41:5000/library/ndk/infra-manager \
                --set infraManager.tag=1.2.0 \
                --set kubeRbacProxy.repository=10.38.106.41:5000/library/ndk/kube-rbac-proxy \
                --set kubeRbacProxy.tag=v0.19.0 \
                --set kubectl.repository=10.38.106.41:5000/library/ndk/kubectl \
                --set kubectl.tag=1.32.3 \
                --set jobScheduler.repository=10.38.106.41:5000/library/ndk/job-scheduler \
                --set jobScheduler.tag=1.2.0 \
                --set tls.server.enable=false \
                --set config.secret.name=nutanix-csi-credentials \
                --set imageCredentials.imagePullSecretName=harbor-registry-credentials

            *   kubectl get svc ndk-intercom-service -n ntnx-system

                NAME                   TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)          AGE
                ndk-intercom-service   LoadBalancer   10.100.66.84   10.38.106.142   2021:32369/TCP   151m

3.  Deploy nginx with pvc application ion your 1st managed cluster (DC)

    - From Code Server (Bastion VM)
        
        wget https://github.com/Ariefpri1122/nkp-airgap/blob/main/src/calm/NKP/nginx-vpc.yaml
        kubectx --> choose your 1st managed cluster (DC)
        kubectl apply -f nginx-vpc.yaml


    -   From your kubernetes dasboard in 1st managed cluster (DC)
        *   Choose yourname-nginx namespace
        *   Click pods from the left menu
        *   Click 3 dot menu (on the right side) of your nginx pod - exec
                    apt-get update
                    apt-get install vim -y
        *   vi /usr/share/nginx/html/index.html
        *   Modify existing content:

<html>
<body>
<h1><center>
WELCOME TO ARIEF'S NDK REPLICATION LAB
</h1></center>
</body>
</html>

        * Examine the website by clicking its service external IP in kubernetes dashboard

4.  Create Storage Cluster Instance on Your 1st Managed Cluster (DC)

    - Get UUID of your Prism Element (PE) and Prism Central (PC) 
            *   ssh from you CVM (PE)   -->   ncli cluster info

                    Cluster Uuid    :   00063b3d-9795-f221-6161-7cc255051991

            *   ssh from your PCVM (PC) -->   ncli cluster info

                    Cluster Uuid    :   68aedcbb-09e6-4acf-9d89-1ffde29f9f12

    -   Create the storage cluster CRD

apiVersion: dataservices.nutanix.com/v1alpha1
kind: StorageCluster
metadata:
    name: my-ndk-storage-cluster
spec:
    storageServerUuid: 00063b3d-9795-f221-6161-7cc255051991
    managementServerUuid: 68aedcbb-09e6-4acf-9d89-1ffde29f9f12

5.  Creating a Remote CR on the 1st Managed Cluster (DC)

apiVersion: dataservices.nutanix.com/v1alpha1
kind: Remote
metadata:
  name: remote-cr-drc
spec:
  clusterName: remote-cr-drc
  ndkServiceIp: 10.38.106.142 
  ndkServicePort: 2021

    -   kubectx --> choose you 1st managed cluster (DC)
        kubectl get remote -A

        NAME                ADDRESS        PORT   AVAILABLE
        remote-cr-primary   10.38.106.142   2021   True

6.  Create Storage Cluster Instance on Your 2nd Managed Cluster (DRC)

    - Get UUID of your Prism Element (PE) and Prism Central (PC) 
            *   ssh from you CVM (PE)   -->   ncli cluster info

                    Cluster Uuid    :   00063b3d-9795-f221-6161-7cc255051991

            *   ssh from your PCVM (PC) -->   ncli cluster info

                    Cluster Uuid    :   68aedcbb-09e6-4acf-9d89-1ffde29f9f12

    -   Create the storage cluster CRD

apiVersion: dataservices.nutanix.com/v1alpha1
kind: StorageCluster
metadata:
    name: my-ndk-storage-cluster
spec:
    storageServerUuid: 00063b3d-9795-f221-6161-7cc255051991
    managementServerUuid: 68aedcbb-09e6-4acf-9d89-1ffde29f9f12

7.  Creating a Remote CR on the 2nd Managed Cluster (DRC)

apiVersion: dataservices.nutanix.com/v1alpha1
kind: Remote
metadata:
  name: remote-cr-dc
spec:
  clusterName: remote-cr-dc
  ndkServiceIp: 10.38.106.56
  ndkServicePort: 2021

    -   kubectx --> choose you 1st managed cluster (DRC)
        kubectl get remote -A

        NAME                ADDRESS        PORT   AVAILABLE
        remote-cr-primary   10.38.106.56   2021   True

8.  Creating a Replication Target CR on the 1st Managed Cluster (DC)

apiVersion: dataservices.nutanix.com/v1alpha1
kind: ReplicationTarget
metadata:
  name: replication-target-drc
  namespace: arief-nginx
spec:
  namespaceName: arief-nginx
  remoteName: remote-cr-drc
  serviceAccountName: default

        kubectl get replicationtargets -n arief-nginx

        NAME                     REMOTE-NAME     REMOTE-NAMESPACE   AVAILABLE
        replication-target-drc   remote-cr-drc   arief-nginx        True

9. Creating Application CR and Snaphot in 1st Managed Cluster (DC)

apiVersion: dataservices.nutanix.com/v1alpha1
kind: Application
metadata:
  name: nginx-only
  namespace: arief-nginx
spec:
  applicationSelector:
    resourceLabelSelectors:
      - labelSelector:
          matchLabels:
            k8s-app: arief-nginx-pvc

apiVersion: dataservices.nutanix.com/v1alpha1
kind: ApplicationSnapshot
metadata:
  name: ndk-snapshot
  namespace: arief-nginx
spec:
  source:
    applicationRef:
      name: nginx-only
  expiresAfter: 180m

kubectl get Application -n arief-nginx
kubectl get applicationsnapshot -n arief-nginx  # --> Wait until ready-to-use true

NAME           AGE   READY-TO-USE   BOUND-SNAPSHOTCONTENT                      SNAPSHOT-AGE
ndk-snapshot   13s   true           asc-fd5c9688-d819-41ec-a7ab-8adb170af72a   8s

kubectl get applicationsnapshot -n arief-nginx ndk-snapshot  -o yaml -w

*** script to cleanup ***
kubectl delete Application -n arief-nginx nginx-only
kubectl delete applicationsnapshot -n arief-nginx ndk-snapshot

10. Replication Applicatoin Snapshot from DC to DRC

apiVersion: dataservices.nutanix.com/v1alpha1
kind: ApplicationSnapshotReplication
metadata:
  name: nginx-only-replication
  namespace: arief-nginx
spec:
  applicationSnapshotName: ndk-snapshot
  replicationTargetName: replication-target-drc

kubectl get applicationsnapshotreplication -n arief-nginx

NAME                     AVAILABLE   APPLICATIONSNAPSHOT   REPLICATIONTARGET        AGE
nginx-only-replication   True        ndk-snapshot          replication-target-drc   24s

*** script to cleanup ***
kubectl delete applicationsnapshotreplication -n arief-nginx nginx-only-replication

11. Restore Snaphot in 2nd Managed Cluster (DRC)
    
    -   Check snapshot has been replication from 1st managed cluster (DC)

        kubectl get applicationsnapshot -n arief-nginx

        NAME           AGE   READY-TO-USE   BOUND-SNAPSHOTCONTENT                                  SNAPSHOT-AGE
        ndk-snapshot   20m   true           asc-fd5c9688-d819-41ec-a7ab-8adb170af72a-1987f9198a8   20m

    -   Create new resource below:

apiVersion: dataservices.nutanix.com/v1alpha1
kind: ApplicationSnapshotRestore
metadata:
  name: restore-nginx-only
  namespace: arief-nginx
spec:
  applicationSnapshotName: ndk-snapshot

kubectl get ApplicationSnapshotRestore -n arief-nginx

*** script to cleanup ***
kubectl delete applicationsnapshot -n arief-nginx ndk-snapshot
kubectl delete ApplicationSnapshotRestore -n arief-nginx restore-nginx-only


*** C. Multi-CLuster GitOps Application Continues Deployment ***
----------------------------------------------------------------

Examine existing 1st Managed Cluster (DC) 
-----------------------------------------
From NKP Manage-Cluster Dasboard - Your Workspace - Projects - Click your name project, explore the tabs and you will see this setups:

Clusters                        :  Only 1 Cluster listed (your 1st managed cluster (DC) )
Continuous Deployment (CD)      :  online-boutique
Repository URL                  :  https://github.com/nutanixdev/nkp-microservices-demo.git


-   We will add our 2nd Managed Cluster (DRC) as part of this project so it will get same settings including the GitOps Setting
    *   online-boutique will be deployed to your 2nd managed cluster (DRC)

    * Click Edit Project (top right corner)
    * Select your 2nd managed cluster
    * Save
    * Now you will see 2 clusters in your project.

    * Click COntinues Deployment (CD)
    * Click 3 dot menu (right side) of online-boutique GitOps Source
    * CLick RESUME


Check the deployment 
--------------------

Done below steps from 2nd Managed Cluster (DRC)

From Kubernetes Dashboard  -->  Check Deployment, make sure deployment success (Please be aware that creating new project will create new namespace based on your project name)

Troubleshooting redis card pod:
-------------------------------

This error is by desigend.
  - redis-cart error (this is happened because the image is in docker hub, while other deployment's image use gcp artifact with no auth and limit -- while docker hub has maximum limit):

From Code-Server (Bastion VM):
  - kubectx --> choose commander context
  
    export HARBOR_ADDRESS="$(kubectl -n kommander get kommandercluster host-cluster -o jsonpath='{.status.ingress.address}')"
    echo $HARBOR_ADDRESS

Create harbor-registry-credentials manifest in new project namespace:
---------------------------------------------------------------------

kubectx --> choose your 2nd Managed Cluster (DRC)
kubens --> make sure you choose your new project namespace

cat /etc/docker/certs.d/$HARBOR_ADDRESS:5000/ca.crt --> make sure the file exist

REGISTRY_USERNAME="nutanix"
REGISTRY_PASSWORD="nx2Tech974!" #change password to your cluster password

kubectl create secret generic harbor-registry-credentials \
--from-literal username=$REGISTRY_USERNAME \
--from-literal password=$REGISTRY_PASSWORD \
--from-file=/etc/docker/certs.d/$HARBOR_ADDRESS:5000/ca.crt

kubectl get secrets --> "harbor-registry-credentials" secret created

Modify RedisCart Deployment to fix the problem:
-----------------------------------------------
From NKP Dashboard - your workspace - Projects - your project - Continues Deployment (CD) :
  - Click 3 button menu from online-boutique GitOps Source
  - Click Suspend

From Kubernetes Dashboard:
  - Modify Redist-Card Deployment:
      * change the image location to --> image: 10.38.106.41:5000/proxy_cache/redis:alpine #Change the IP Address to your harbor IP

add below config (After dnspolicy): *** MAKE SURE imagePullSecrets IN THE SAME INDENT WITH dnspolicy ***
      imagePullSecrets:
        - name: harbor-registry-credentials

Click Update Button

* Make sure deployment Running (Click Restart from 3 menu button on redis-cart deployment)
* For curousity open your harbor dashboard - Projects - proxy_cache ---> there is redis there in the repo with 1 pull

Go to Services - frontend-external --> click the external end points IP Address (make sure application shown)
